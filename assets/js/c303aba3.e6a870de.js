"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[72047],{28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>h});var i=n(96540);const a={},o=i.createContext(a);function s(e){const t=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function h(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(o.Provider,{value:t},e.children)}},89221:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>h,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"insights_and_knowledge/modern_dice","title":"Introduction to Modern DICE","description":"Here is the transcript of our knowledge sharing session on modern dice: <a","source":"@site/../docs/insights_and_knowledge/modern_dice.md","sourceDirName":"insights_and_knowledge","slug":"/insights_and_knowledge/modern_dice","permalink":"/docs/insights_and_knowledge/modern_dice","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"modern_dice","title":"Introduction to Modern DICE"},"sidebar":"main","previous":{"title":"Windows Cheat Sheet","permalink":"/docs/developers/windows_cheat_sheet"}}');var a=n(74848),o=n(28453),s=n(29030);const h={id:"modern_dice",title:"Introduction to Modern DICE"},r="Introduction to Modern Dice",c={},l=[{value:"What is Dice?",id:"what-is-dice",level:2},{value:"Dice vs. Standard Programming Functions",id:"dice-vs-standard-programming-functions",level:2},{value:"Incremental Computation in Dice",id:"incremental-computation-in-dice",level:2},{value:"Optimization with Versioning",id:"optimization-with-versioning",level:2},{value:"The \u201cDistributed\u201d Part",id:"the-distributed-part",level:2},{value:"Naive Approach: Initial Directory Traversal in Rust",id:"naive-approach-initial-directory-traversal-in-rust",level:2},{value:"Dice Caching for Improved Efficiency",id:"dice-caching-for-improved-efficiency",level:2},{value:"Implementing Dice Node for Caching Word Count",id:"implementing-dice-node-for-caching-word-count",level:2},{value:"Updating the Recursive Function for Cached Word Count",id:"updating-the-recursive-function-for-cached-word-count",level:2},{value:"Limitations in Parallelization and Directory Walk",id:"limitations-in-parallelization-and-directory-walk",level:2},{value:"Incremental Caching Benefits and Drawbacks",id:"incremental-caching-benefits-and-drawbacks",level:2},{value:"Optimizing Recursive Spawning and Merging",id:"optimizing-recursive-spawning-and-merging",level:2},{value:"Enhanced Caching at Directory Level",id:"enhanced-caching-at-directory-level",level:2},{value:"Final Optimizations Using Early Cutoff",id:"final-optimizations-using-early-cutoff",level:2},{value:"Summary: Efficient Caching with Early Cutoff",id:"summary-efficient-caching-with-early-cutoff",level:2},{value:"Key Lookup and Version Management",id:"key-lookup-and-version-management",level:2},{value:"Managing Computed State and the Role of Modern Dice",id:"managing-computed-state-and-the-role-of-modern-dice",level:2},{value:"Dependency Checking with Check Depths",id:"dependency-checking-with-check-depths",level:2},{value:"Optimizing Dependency Checks for Performance",id:"optimizing-dependency-checks-for-performance",level:2},{value:"Benefits of Non-Speculative Dependency Checks",id:"benefits-of-non-speculative-dependency-checks",level:2},{value:"Overview of Parallel Processing and Task Management",id:"overview-of-parallel-processing-and-task-management",level:2},{value:"Caching Mechanism and State Management",id:"caching-mechanism-and-state-management",level:2},{value:"Processing a Compute Request",id:"processing-a-compute-request",level:2},{value:"Function of the Async Evaluator",id:"function-of-the-async-evaluator",level:2},{value:"Handling Cache Misses",id:"handling-cache-misses",level:2},{value:"Handling Invalidated Values",id:"handling-invalidated-values",level:2},{value:"Injected Keys and Their Impact",id:"injected-keys-and-their-impact",level:2},{value:"User Data in Dice",id:"user-data-in-dice",level:2},{value:"Keys and Specific Computation Targets",id:"keys-and-specific-computation-targets",level:2},{value:"Challenges in Key Equality and State Tracking",id:"challenges-in-key-equality-and-state-tracking",level:2},{value:"Avoiding Untracked Data Flows in Keys and Values",id:"avoiding-untracked-data-flows-in-keys-and-values",level:2},{value:"Challenges in Introducing New Data",id:"challenges-in-introducing-new-data",level:2}];function d(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",p:"p",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"introduction-to-modern-dice",children:"Introduction to Modern Dice"})}),"\n",(0,a.jsxs)(t.p,{children:["Here is the transcript of our knowledge sharing session on modern dice: ",(0,a.jsx)("a",{href:(0,s.default)("/assets/Modern_DICE.pdf"),target:"\\_blank",children:"Download\nSlides"})]}),"\n",(0,a.jsx)(t.p,{children:"I will be talking about modern Dice today. I\u2019ll have to get through. I\u2019ll try to\nbe good with time. So I\u2019ll talk a little bit about what Dice is, show how to use\nit, talk a little bit about the internals and what the modern Dice part of that\nis."}),"\n",(0,a.jsx)(t.h2,{id:"what-is-dice",children:"What is Dice?"}),"\n",(0,a.jsx)(t.p,{children:"So first, what is Dice? Dice is, you know\u2026 We first named this before we started\nthe buck2 word; we call it Distributed Incremental Computation Engine. So what\ndoes that mean? Computation Engine part. This is, with Dice, you configure this\nwith Dice by sort of providing us with leaf data and then define a set of\nfunctions that the engine is going to manage for you, right? And then you make a\nrequest. You know, compute function two of A, say, and it will, you know,\nthat\u2019ll depend on other calls and down to leaf data."}),"\n",(0,a.jsx)(t.h2,{id:"dice-vs-standard-programming-functions",children:"Dice vs. Standard Programming Functions"}),"\n",(0,a.jsx)(t.p,{children:"You know, at this point, like, is this really any different than just like\nPython, right? Python, you define a bunch of functions, you call them, and it\ndeals with, you know, calling the other functions. At this point, not really,\nright? Like, yeah, it does a little bit. So it\u2019ll spawn this work in parallel,\nright? It\u2019ll share work if multiple nodes are requesting it. But really, the\ninteresting parts, I think, when you get to the incremental, the incremental\ncomputation engine."}),"\n",(0,a.jsx)(t.h2,{id:"incremental-computation-in-dice",children:"Incremental Computation in Dice"}),"\n",(0,a.jsxs)(t.p,{children:["So with this, we can invalidate these leaf nodes, right? So invalidate"," ",(0,a.jsx)(t.a,{href:"https://internalfb.com/L2",children:"L2"})," ","down\nthere at the bottom. Dice tracks dependencies, and it manages this invalidation\nfor us, right? So it\u2019s going to invalidate all those reverse dependencies of"," ",(0,a.jsx)(t.a,{href:"https://internalfb.com/L2",children:"L2"}),".","\nThen, say, you compute a new function up here, right? It\u2019s going to, you know,\nupdate the node\u2019s values for everything between, right? So it does, you know, it\ndoes efficient recomputation, right? It only recomputes the nodes that have been\ninvalidated and need to be recomputed."]}),"\n",(0,a.jsx)(t.h2,{id:"optimization-with-versioning",children:"Optimization with Versioning"}),"\n",(0,a.jsxs)(t.p,{children:["Has this important optimization that if, as we sort of recompute up one of these\nnodes, recomputes the nodes that are invalidated, it only recomputes the nodes\nthat are invalidated. It only recomputes the nodes that are invalidated. It only\nrecomputes the nodes that are invalidated. It only recomputes the same value it\nhad previously, right? We record that. We, you know, the values are recorded\nwith version numbers. We store the sort of last version that you were valid at.\nAnd then when recomputing a node, even if it was, if it was invalidated, if all\nof its dependencies are basically recomputed to the same values, that node will\nskip recomputation and just say, oh, it still has the same value as it had at\nB2. And so that can, that can, you know, maybe here"," ",(0,a.jsx)(t.a,{href:"https://internalfb.com/L2",children:"L2"})," ","changed, but X2 and X3\ndidn\u2019t. And then all the rest would not actually have to do recomputation."]}),"\n",(0,a.jsx)(t.h2,{id:"the-distributed-part",children:"The \u201cDistributed\u201d Part"}),"\n",(0,a.jsx)(t.p,{children:"The distributed part, there are people who are trying, who have tried to\nconvince us to change the D. It\u2019s not distributed at this point. You know, it\u2019s\nkind of like FSD, right? It\u2019s an aspirational naming. So we\u2019ll get to that.\nWe\u2019ll get to that maybe next year, next year\u2019s talk."}),"\n",(0,a.jsx)(t.h1,{id:"example-walkthrough-word-count-in-recursive-directory",children:"Example Walkthrough: Word Count in Recursive Directory"}),"\n",(0,a.jsx)(t.p,{children:"So, yeah, let\u2019s, let\u2019s work through an example. This is, you know, I\u2019m just, you\nknow, this is sort of a toy example. Let\u2019s, let\u2019s do like word counts of a\nrecursive directory, right? Say we\u2019ve got, you know, a couple, a couple of\nfunctions for us already, reading files, listing directories, getting word\ncounts for a string. Pretend they take this long and we\u2019ll, we\u2019ll talk about\nsort of the time it takes to, with, with different approaches."}),"\n",(0,a.jsx)(t.h2,{id:"naive-approach-initial-directory-traversal-in-rust",children:"Naive Approach: Initial Directory Traversal in Rust"}),"\n",(0,a.jsx)(t.p,{children:"Yeah. So this is, this is a pseudo Rust. There\u2019s the, you know, Rust has some\nrequired things that make it a little more proposed. I cut those out. But this\nis, this is just sort of a naive approach to this, right? It\u2019s not totally\nnaive, right? So we\u2019re walking, we\u2019re walking the directory and we\u2019re spawning\noff the sort of expensive get word count, right? So this is all done in\nparallel. Join the counts at the end and then merge that, right?"}),"\n",(0,a.jsx)(t.h2,{id:"dice-caching-for-improved-efficiency",children:"Dice Caching for Improved Efficiency"}),"\n",(0,a.jsx)(t.p,{children:"So, you know, through, through a couple of scenarios together here, right? The\ncold one, and then a couple of incremental scenarios in this case, right?\nThere\u2019s no incrementality. It\u2019s just normal Rust code. So anytime you have to\ncompute it, it\u2019s going to pay the full cost. I was, you know, I tried to try to\nget the numbers here, correct, but I wasn\u2019t super careful. So if, if I\u2019m off a\nlittle bit. So that\u2019s, you know, don\u2019t worry about it, but yeah, so, so the time\nhere, right? You have, you have something like a, you know, in parallel, you\nhave like a thousand seconds of, of directory traversal and 10,000 or something\nof getting word counts, something like that."}),"\n",(0,a.jsx)(t.h2,{id:"implementing-dice-node-for-caching-word-count",children:"Implementing Dice Node for Caching Word Count"}),"\n",(0,a.jsx)(t.p,{children:"Okay. So the, the first, the obvious one, if, you know, we have, we have dice,\nit does, you know, it\u2019s supposed to do caching for us, let\u2019s cache that\nexpensive get word count. And so this is, this is what it was. Look like, uh,\nwith a lot of, with a lot of boilerplate removed of sort of introducing a dice\nnode to, to cache that word count, right? So we introduced a word count key, and\nthen we have this key implementation. This key implementation is like the\nfunctions I was referring to in the dice configuration, right? Which tells you\nhow to take, uh, a key is like the, you know, the input to the function. And\nthen this compute call is, is the function, this is going to be what dice is\ncaching and right. All it does is the get word count, uh, which are."}),"\n",(0,a.jsx)(t.h2,{id:"updating-the-recursive-function-for-cached-word-count",children:"Updating the Recursive Function for Cached Word Count"}),"\n",(0,a.jsx)(t.p,{children:"Dice read file in here. So, uh, imagine that, uh, read file is one of those\nleafs in the, in the image report, right? Uh, and so then using that, we can\nupdate our word count recursive, uh, right. I circled the differences here,\nright? We are pushing, we\u2019re collecting a list of files instead of list of, uh,\nget word count futures. And then at the end, we do this instead of futures join\nall we\u2019re doing this CTX that join all on the dice computations."}),"\n",(0,a.jsx)(t.h2,{id:"limitations-in-parallelization-and-directory-walk",children:"Limitations in Parallelization and Directory Walk"}),"\n",(0,a.jsx)(t.p,{children:"Right. Um, you know, one of the things here, unlike the previous one, the\nprevious one could start spawning the work early, right? In this case, you see,\nactually, we have to do the full directory walk before we spawn the work. Uh,\nand that\u2019s due to the dice computations here. If you look at the function\nsignature, it takes a new reference. Um, if you are working rust, you know, you\ncan only have sort of one of those at a time. So we wouldn\u2019t be able to spawn\nthose off early, uh, we don\u2019t want to be able to spawn off one."}),"\n",(0,a.jsx)(t.h2,{id:"incremental-caching-benefits-and-drawbacks",children:"Incremental Caching Benefits and Drawbacks"}),"\n",(0,a.jsx)(t.p,{children:"So, so, okay, great. So we, we, in this case, right. So because of that, uh,\nneeding to walk first before spawning any work, right. The cold case is actually\nslower. Uh, but all, all the add file add or fixed type will rename file. I\u2019ll\nget much faster. Right. So if you think, if you think through, okay, so the, the\nget word count is going to be cashed. If you add a file, um, the work that\u2019s\ngoing to need to be redone is like, yeah, we have to. First that directory, we\nsort of request out all the, all the word counts and we only actually have to\nrecompute the one new one."}),"\n",(0,a.jsx)(t.h2,{id:"optimizing-recursive-spawning-and-merging",children:"Optimizing Recursive Spawning and Merging"}),"\n",(0,a.jsx)(t.p,{children:"Uh, and then actually the merge at the end is another, is another thousand\nseconds, uh, because it\u2019s emerging, you know, because of these costs that I,\nthat I threw out there. Uh, so okay. Uh, let\u2019s, let\u2019s see, let\u2019s fix the, let\u2019s\nfix the, just the first, like getting it all to spawn in parallel, right? So we\ncan do that. This is just changing the word count to, um, sort of spawn out\nthese red spawned out, recursive, right? So, so we spawn out for it, for a\ndirectory, we list it and sub directories, we spawn out the, the recurse for\nthat and then, and then merge it."}),"\n",(0,a.jsx)(t.h2,{id:"enhanced-caching-at-directory-level",children:"Enhanced Caching at Directory Level"}),"\n",(0,a.jsx)(t.p,{children:"Right. So this both, uh, does it all in parallel, but also the merge at the end\nis not all million items. It\u2019s, you know, just the items for one directory kind\nof merge them as we, as we recurse up. Right. Uh, so. I think this is, wait, did\nI say that wrong? Uh, we don\u2019t merge them as we recurse up in this case. Do we?\nAll right. Uh, these, these numbers are a little wrong. I think they should do\nmostly a hundred seconds. Um, but again, so this is, this is getting a bit\nbetter."}),"\n",(0,a.jsx)(t.h2,{id:"final-optimizations-using-early-cutoff",children:"Final Optimizations Using Early Cutoff"}),"\n",(0,a.jsx)(t.p,{children:"Uh, but we can, we can do even better than that, right? We can, we can put on\ndice, the word counts for each of the directories, right. Um, or recursive for\neach directory. And so this looks very much like the, the. Caching it for a\nfile, right. So it looks the same, uh, I kind of call it a word comp recursive\nbefore here, which is going to be right here, right? And so again, we don\u2019t\nchange much. We switched to a dice list directory. So we\u2019re caching the, just\nthe directory listing and then a dice, uh, work count. The recursively cache\ndice word count."}),"\n",(0,a.jsx)(t.h2,{id:"summary-efficient-caching-with-early-cutoff",children:"Summary: Efficient Caching with Early Cutoff"}),"\n",(0,a.jsx)(t.p,{children:"Uh, and now our timings get much better right now. Right now, we just, in each\nof these cases, I think it ends up being, we are recomputing one, one, uh, sort\nof file word count and then just merging them up a set of directories. We can do\na little bit better here, right? So in the fixed typo and rename file case,\nright? The actual like word counts of that file or that directory don\u2019t change.\nUm, and so that\u2019s, that\u2019s where we should be able to get this early cutoff and\nthe way. We do that is if you go back to the key implementations, they have this\nequality function and you\u2019re right. So we implement the equality function and\nthen dice while it\u2019s doing the recomputation, we\u2019ll, we\u2019ll apply this early\ncutoff optimization for us. Quality here is simple. And so then we get sort of\nour best case scenarios for, for each, for each case. Okay. So that, that gets\nus through the example."}),"\n",(0,a.jsx)(t.h1,{id:"core-state-thread-and-key-operations",children:"Core State Thread and Key Operations"}),"\n",(0,a.jsx)(t.p,{children:"Okay. Uh, great. Looking into the core state thread, really, there are just\nthree main messages it receives. There\u2019s actually a list of like a dozen or so,\nbut three main ones are important. The first is an update state message, which\nis used for injecting values at the leaves or invalidating them. For each of\nthese, the core state will invalidate the node, traverse the dependencies, and\nincrement its version if anything changed."}),"\n",(0,a.jsx)(t.h2,{id:"key-lookup-and-version-management",children:"Key Lookup and Version Management"}),"\n",(0,a.jsx)(t.p,{children:"The next key operation is look_up_key, which finds a node in the map. If it\u2019s\nthere and valid, it returns a match; if it\u2019s missing, it returns a compute, or\nif it\u2019s invalidated, it triggers dependency checks. Versions increment with each\nchange, and every computation starts at the newest version, maintaining a\nhistory of node states to track when they were last computed or invalidated."}),"\n",(0,a.jsx)(t.h2,{id:"managing-computed-state-and-the-role-of-modern-dice",children:"Managing Computed State and the Role of Modern Dice"}),"\n",(0,a.jsx)(t.p,{children:"After a compute task finishes, the updated computed column records new values\nand dependencies. This structure, where a single-threaded core state manages the\nmain state, is one of the major changes with Modern Dice. Previously, the main\nstate was managed within the async evaluator with fine-grained locks, which\nbecame complex to manage, motivating the shift to single-threaded state\nmanagement."}),"\n",(0,a.jsx)(t.h2,{id:"dependency-checking-with-check-depths",children:"Dependency Checking with Check Depths"}),"\n",(0,a.jsx)(t.p,{children:"Let\u2019s talk about check_depths and how it works. Imagine this as our recursive\nword count example. If a file or directory changes, the core state returns\ncheck_depths. In the old Buck approach, all dependencies were checked in\nparallel, exiting early if a change was detected, but this could spawn\nunnecessary tasks. For instance, if hacking.md was deleted, an invalid key\nrequest could trigger a panic, as seen in Buck\u2019s previous behavior."}),"\n",(0,a.jsx)(t.h2,{id:"optimizing-dependency-checks-for-performance",children:"Optimizing Dependency Checks for Performance"}),"\n",(0,a.jsx)(t.p,{children:"The first fix was to check dependencies in order, but this was too slow, as it\nled to single-threaded recomputation until changes were found. In Modern Dice,\nwe use a different approach. With CTX.joinAll, each dependency computation runs\nin its own dice computation. Each future created gets its own dependency\ntracking, and when joinAll finishes, all dependencies merge into the outer dice\ncomputation as a series-parallel graph, allowing for both ordered and parallel\ncomputations efficiently."}),"\n",(0,a.jsx)(t.h1,{id:"series-parallel-graph-structure-for-dependency-tracking",children:"Series-Parallel Graph Structure for Dependency Tracking"}),"\n",(0,a.jsx)(t.p,{children:"All right. This is kind of a picture of what a series parallel graph looks like.\nThis isn't related to the recursive word count key stuff. I think maybe it would\nbe nice if it were, if I had an example of the code here to show it. But, right.\nSo the idea here would be, you know, this looks like, you know, a computeK1,\ncomputeK2, and then a joinAll splits into three, you know, three inner things.\nThis could be a join3, right? And so then looking through the top ones, a\ncomputeK3, and then another, say, a join2, et cetera. And then the red dots are\nsort of just indicating when the join finishes. Right. So, like, that's the\nstructure. That's the structure of depths that we are, that we're recording when\nwe talk about recording depths."}),"\n",(0,a.jsx)(t.h2,{id:"benefits-of-non-speculative-dependency-checks",children:"Benefits of Non-Speculative Dependency Checks"}),"\n",(0,a.jsx)(t.p,{children:"We record them in, like, it's like two flat lists, right? It's recorded as a\nflat list of keys and then a flat list of descriptors that describe how to\nunderstand those keys as this graph. And so the checkDepthSpeed3 looks a little\nbit more like this. You know, this kind, you know. We iterate through the, we\niterate through sort of the series nodes in the graphs, right? Checking them in\norder, exiting out as soon as we get one that's changed. Parallel node, parallel\nnodes, we still do the spawn. Maybe I missed a spawn here, but I have the\njoinAll. And it turns out, like, with this, right, we won't ever request a key\nthat the normal compute wouldn't also request. Right. We've sort of tracked the\nintra-node data flow or data dependencies, rather. And so one of the great\nthings that means is that, like, previously, we would have to cancel, right? We\ndo these checkDepths. It spawns off, you know, it spawns off 10 nodes. Those\nspawn off a bunch more. And then we find one change, and we, like, cancel all\nthat work because, you know, it's all speculative. At this point, it's, like,\nnot really speculative, right? We know. Even if this. The depth changes, like,\ngreat. That's fine. We do have to redo the compute, and we're going to start\nredoing the compute. But doing that compute is going to request all the same\nthings that we're doing here because the things up to what we're doing here have\nbeen, you know, it will get the same results that it previously had gotten. And\nso then we'll do the next things the same. Assuming that your compute is not\nnon-deterministic, which we've been pretty good about. That's not. We've been\nbad about other things, but pretty good about having deterministic piece."}),"\n",(0,a.jsx)(t.h1,{id:"internal-workflow-of-dice-computations",children:"Internal Workflow of Dice Computations"}),"\n",(0,a.jsx)(t.p,{children:"Let me talk a bit about how this works. So this is, you know, here\u2019s, here\u2019s the\npart of the get word count. I cut out the rest, but this is, you do CTX.compute\non this key. Uh, what\u2019s it doing? Uh, it behaves sort of just like you had\ncalled compute, you know, the word count key\u2019s compute code. But internally\nthere\u2019s a lot going on."}),"\n",(0,a.jsx)(t.h2,{id:"overview-of-parallel-processing-and-task-management",children:"Overview of Parallel Processing and Task Management"}),"\n",(0,a.jsx)(t.p,{children:"So this is like the rough sketch of this, right? We have a Tokio runtime doing a\nwhole bunch of things in parallel. We have a whole bunch of different sorts of\ncomputes going on, and each one of them is holding onto one of these dice\ncomputations, which has a depth recording when you call compute. This goes to an\nasync evaluator, which we have one per transaction\u2014essentially one per larger\ncomputation, like one per buck command."}),"\n",(0,a.jsx)(t.h2,{id:"caching-mechanism-and-state-management",children:"Caching Mechanism and State Management"}),"\n",(0,a.jsx)(t.p,{children:"The async evaluator maintains a shared cache of keys to tasks. A task is\nessentially about getting the result. The first time we get a compute request,\nit spawns the computation and communicates with the core state thread, where the\nmain cache and state management occur over time. The shared cache is more\ntemporary, quickly sharing work when multiple requests hit the same key, but the\ncore state thread manages the main state."}),"\n",(0,a.jsx)(t.h2,{id:"processing-a-compute-request",children:"Processing a Compute Request"}),"\n",(0,a.jsx)(t.p,{children:"Let\u2019s work through how this goes: Dice computations hold the reference to the\ncompute call, which goes to the async evaluator. The evaluator returns a shared\nfuture; then, we await on it, record the dependency, and return the result to\nthe user code. For Dice computations, that\u2019s it. It\u2019s straightforward."}),"\n",(0,a.jsx)(t.h2,{id:"function-of-the-async-evaluator",children:"Function of the Async Evaluator"}),"\n",(0,a.jsx)(t.p,{children:"The async evaluator does a get_or_insert on its map, spawning a task for each\nrequested key once per transaction. The compute task sends a message to the\nstate thread to look up the key in the state cache. If the state has it cached,\nit returns a match to the compute task, which satisfies the future, and anyone\nawaiting it receives the result."}),"\n",(0,a.jsx)(t.h2,{id:"handling-cache-misses",children:"Handling Cache Misses"}),"\n",(0,a.jsx)(t.p,{children:"If the key isn\u2019t in the cache, the core state first returns a compute, then the\ncompute task calls the key\u2019s compute implementation, retrieves the result, and\nsends it back to the core state. The core state may return a different result\nthan what was sent by the compute task due to reference equality requirements.\nIt\u2019s essential to return the result instance from the core state, even if\nlogically equal."}),"\n",(0,a.jsx)(t.h2,{id:"handling-invalidated-values",children:"Handling Invalidated Values"}),"\n",(0,a.jsx)(t.p,{children:"The final case is when the core state had a cached value that became\ninvalidated. Here, the core state returns check_deps with information about\ndependencies that need re-checking. If dependencies have changed, the compute\ntask recomputes following the same path. If dependencies are unchanged, a\nmessage indicating no changes is sent, providing the same result back."}),"\n",(0,a.jsx)(t.h1,{id:"data-flow-in-dice-computation",children:"Data Flow in Dice Computation"}),"\n",(0,a.jsx)(t.p,{children:"So, yeah. A little bit about how data flows overall into the Dice computation,\nright? Like, there\u2019s three ways that it\u2019s intended to flow in, right? We have\ninjected keys sort of down the leaves at the bottom of the graph as I drew it,\nright? This is going to be used for like global data for things that are in like\none state for a particular computation. A really good example of this previously\nwas buck config, right? The whole buck config would be computed and then put in\na leaf. We\u2019ve actually now, the computation is actually now split and done\npartially on Dice, but still the main inputs to buck config are leaves on the\ngraph."}),"\n",(0,a.jsx)(t.h2,{id:"injected-keys-and-their-impact",children:"Injected Keys and Their Impact"}),"\n",(0,a.jsx)(t.p,{children:"The buck out path, I think, is in an injected key. I think the path to the\nprelude, things like this, where it\u2019s like, yeah, we only have one of those.\nIt\u2019s not really going to, you know, we don\u2019t expect the work. I shouldn\u2019t say\nthat if the workflow involves like changing one of these a lot, using an\ninjected key may not be the best thing, right? Because if say something like\nbuck config, right? If you change buck config, it invalidates basically\neverything. And like the workflows for users require changing buck config today.\nAnd so that\u2019s like an unfortunate aspect of the buck config design that sort of\nrequires that, and like, it\u2019s not, you know, injected key for that. Like, yes,\nit\u2019s how you have to do it because that\u2019s the design of buck config, but it\u2019s\nnot great for the workflow."}),"\n",(0,a.jsx)(t.h2,{id:"user-data-in-dice",children:"User Data in Dice"}),"\n",(0,a.jsx)(t.p,{children:"The buck out path, right? We don\u2019t, you know, that\u2019s not changing during normal\nwork. And so like, if the cost of changing that is high, that\u2019s like, you know,\nfiles and directories that act like this, they\u2019re not injected. We don\u2019t inject\nthe values, but we do like invalidate them. Kind of similar. And like, that is\nkind of where data comes in from outside the graph, user data. User data is just\ndata that we stick on Dice to give us access to things that are useful to have\naccess to, but aren\u2019t meaningful to the computation. The best example of this is\nthe event dispatcher, right? Every buck command creates an event dispatcher to\nsort of get events back out to the client or to the log. And we, you know, we\nput this on the per transaction user data, and keys just access this as much as\nthey want."}),"\n",(0,a.jsx)(t.h2,{id:"keys-and-specific-computation-targets",children:"Keys and Specific Computation Targets"}),"\n",(0,a.jsx)(t.p,{children:"Dice doesn\u2019t really track accesses to user data. And so it\u2019s really only\nintended for things that aren\u2019t going to affect the computation. And then the\nother way that you might not think about data getting into the computation, but\nit really is, is in the data in the keys themselves. For a build, you know, this\nmight be as little as just what the target is, right. In some sense, Dice could\ncompute anything, right? It\u2019s like this large infinite graph of possibilities,\nand you\u2019re telling it which specific ones you want. I think what people might\nthink of more as data are BXL files. So BXL files end up in a key for like a BXL\ncomputation. Great. And from those, we form these huge computations of work,\nright?"}),"\n",(0,a.jsx)(t.h1,{id:"best-practices-and-key-pitfalls",children:"Best Practices and Key Pitfalls"}),"\n",(0,a.jsx)(t.p,{children:"Great, a little bit about, I don\u2019t know, best practices, things to keep in mind.\nI\u2019m not gonna actually, I\u2019m not gonna go through all of these; maybe I\u2019ll leave\nthis slide up when we get to questions. But the biggest things are like key in\nvalue, key equality especially is where we\u2019ve sort of had the most issues of\ngetting it incorrect. This is from, you know, one sort of mistake we make is we\nwill have a tendency, I think, to think about things in terms of like what\nstates they can be within a command, right? And so within a command, a\nconfigured target node, right? The configured target label is like a unique\nidentifier for one specific configured target node, right? That\u2019s true within a\ncommand, but when you start comparing keys or values across commands, that\u2019s no\nlonger true."}),"\n",(0,a.jsx)(t.h2,{id:"challenges-in-key-equality-and-state-tracking",children:"Challenges in Key Equality and State Tracking"}),"\n",(0,a.jsx)(t.p,{children:"If you know, you might make a change, right? You might make a change to\nsomething that\u2019s producing a new configured target node for the same configured\ntarget label. And Dice is gonna want to be able to compare both keys and values\nacross those states. And so that\u2019s sort of a pitfall we\u2019ve fallen into. Another\nis allowing data flow that is not tracked by Dice, right? So if in your key or\nin your value, you have a mutex, and you have some mutable state in there, that\ncan allow data to flow in ways that Dice isn\u2019t tracking, which can cause bad\nbehavior."}),"\n",(0,a.jsx)(t.h2,{id:"avoiding-untracked-data-flows-in-keys-and-values",children:"Avoiding Untracked Data Flows in Keys and Values"}),"\n",(0,a.jsx)(t.p,{children:"An interesting one we had was like a lazily initialized lock, either a once lock\nor a lazy if people know the Rust concepts. In one of these, it was in a key,\nand we thought that we were doing it correctly, but we basically allowed data\nflow that Dice wasn\u2019t tracking correctly. Those are the big things\u2014getting\nequality right is critical."}),"\n",(0,a.jsx)(t.p,{children:"User Data Considerations and Proper Data Flow"}),"\n",(0,a.jsx)(t.p,{children:"And then, like, don\u2019t put things in user data that are essential to the\ncomputation, right? They have to get into the computation in another way, often\njust on a leaf in the graph. Often, the things that people want to put in user\ndata end up being more appropriate just on a leaf in the graph."}),"\n",(0,a.jsx)(t.h2,{id:"challenges-in-introducing-new-data",children:"Challenges in Introducing New Data"}),"\n",(0,a.jsx)(t.p,{children:"Introducing new data that flows through keys can be really hard. You can imagine\nhost info today, available in Starlark, right? You have this host info. You can\nimagine wanting to switch that from being an injected key to being in the key\nitself and flowing down. And what you find is that you have to flow that\nanywhere that a target label goes. Anywhere a target label\u2019s in a key also ends\nup needing that info, but it can be hard to introduce new things there."}),"\n",(0,a.jsx)(t.h1,{id:"conclusion-of-modern-dice-overview",children:"Conclusion of Modern Dice Overview"}),"\n",(0,a.jsx)(t.p,{children:"Okay, that\u2019s it. Modern Dice, quick half-hour overview."})]})}function u(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);